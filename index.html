<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Thought-Retriever: Don't Just Retrieve Raw Data, Retrieve Thoughts</title>

  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous" referrerpolicy="no-referrer" />

  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Thought-Retriever: Don't Just Retrieve Raw Data, Retrieve Thoughts</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Tao Feng<sup>1*</sup>,
              </span>
                <span class="author-block">
                  Pengrui Han<sup>1,2*</sup>,
                </span>
                  <span class="author-block">
                    Guanyu Lin<sup>1,3*</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://www.mit.edu/~geliu/" target="_blank">Ge Liu</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <span class="author-block">
                      <a href="https://cs.stanford.edu/~jiaxuan/" target="_blank">Jiaxuan You</a><sup>1</sup></span>
                    <span class="author-block">
                
                  </div>
             
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>University of Illinois Urbana-Champaign,
                      <sup>2</sup>Carleton College,
                      <sup>3</sup>Carnegie Mellon University
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2402.07456.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>



                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/OS-Copilot/FRIDAY" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.07456" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                  <!-- Benchmark -->
                  <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solid fa-database"></i>
                  </span>
                  <span>AcademicEval</span>
                </a>
              </span>

                  <!-- Demo -->
                  <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa-regular fa-user"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/excel.mov"
        type="video/mp4">
      </video>

    </div>
  </div>
</section> -->
<section class="section hero">  
  <div class="container is-max-desktop">
    We will replace this with our demo videos 
      <div class="columns is-justify-content-center">
        <div class="column is-6"> <!-- 这里设置为占用一半的宽度 -->
          <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="static/videos/excel.mov" type="video/mp4">
          </video> -->
          <div class="video-container">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <source src="static/videos/excel.mov" type="video/mp4">
            </video>
            <p class="has-text-centered"><b>Demo1: Operating Excel Files.</b></p>
          </div>
        </div>
        <div class="column is-6"> <!-- 这里设置为占用一半的宽度 -->
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="static/videos/react_website.mov" type="video/mp4">
          </video>
          <p class="has-text-centered"><b>Demo2: Creating a Webpage.</b></p>
        </div>
    </div>  
      <div class="columns">  
          <div class="column is-6">
              <div class="video-container">  
                <video poster="" id="tree" autoplay controls muted loop height="100%">
            
                  <source src="static/videos/enter_focused_mode.mov"
                  type="video/mp4">
                </video>
                <p class="has-text-centered"><b>Demo3: Entering Focused Mode.</b></p>
              </div>  
          </div>  
          <div class="column is-6">  
              <div class="video-container">  
                <video poster="" id="tree" autoplay controls muted loop height="100%">
            
                  <source src="static/videos/play_mosic.mov"
                  type="video/mp4">
                </video>
                <p class="has-text-centered"><b>Demo4: Playing Music.</b></p>
              </div>  
       
      </div>
    </div>
      <div>
        <p><b>(Stay tuned! More demos are coming soon~)</b></p>
      </div>  
  </div>  
</section>  

<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) have transformed AI research thanks to their powerful internal capabilities and knowledge. However, existing LLMs still fail to effectively incorporate the massive external knowledge when interacting with the world. Although retrieval-augmented LLMs are proposed to mitigate the issue, they are still fundamentally constrained by the context length of LLMs, as they can only retrieve top-K raw data chunks from the external knowledge base which often consists of millions of data chunks. Here we propose Thought-Retriever, a novel model-agnostic algorithm that helps LLMs generate output conditioned on arbitrarily long external data, without being constrained by the context length or number of retrieved data chunks. Our key insight is to let an LLM fully leverage its intermediate thoughts generated when solving past user queries, organizing them in thought memory, and retrieving the relevant thoughts when addressing new queries. Notably, Thought-Retriever can self-evolve through continuous user interactions thanks to the growing number and depth of thoughts. Besides algorithmic innovation, we further meticulously prepare a novel benchmark, AcademicEval, which requires an LLM to faithfully leverage ultra-long context to answer queries based on real-world academic papers. Extensive experiments on AcademicEval and two other datasets validate that Thought-Retriever remarkably outperforms state-of-the-art baselines by achieving a 5%-45% higher win rate. More importantly, we further demonstrate 2 exciting findings: (1) Thought-Retriever can indeed help LLM self-evolve after solving more user queries; (2) Thought-Retriever learns to leverage deeper thoughts to answer more abstract user queries.

          </p>
        </div>

        <!-- <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/figure2.png" alt="MY ALT TEXT"/>
          </div>
          <b class="subtitle is-size-6">
            Thought-Retriever Framework. (a) Thought retrieval: Upon receiving a user query, Thought-Retriever retrieves top-K data
            chunks from the mixture of external knowledge and thought memory based on embedding similarity; (b) Answer generation: The LLM generates the answer for the user query based on the retrieved data chunks; (c) Thought generation: The LLM further generates thought and its confidence based on the user query and the generated answer; (d) Thought memory update: Meaningless and redundant thoughts are removed and the remaining novel thoughts are used to update the thought memory. 
          </b>
        </div> -->

      </div>
    </div>
  </div>
</section>



<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">1. Motivation</h2>
        <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/Motivation.png" alt="MY ALT TEXT"/>
          </div>
          <b class="subtitle is-size-6">
          </b>
        </div>

        <h2 class="title is-4"></h2>
        <div class="content has-text-justified">
          <p>
            
          </p>
        </div>
        
        <div class="content has-text-justified">
          
          <p>For instance, consider the scenario where an LLM can only process two data chunks in its context window, yet <span class="math-container">\( \mathcal{K}_i = \{K_1, K_2, K_3, K_4\} \)</span> are needed to fully answer a query. Traditional methods might retrieve accurately but fail to cover all relevant data, compromising either precision or recall.</p>
          <p>In contrast, Thought-Retriever leverages past LLM thoughts and balances low-level facts and high-levle thoughts to answer user queries, offering a flexible and easy approach to achieve better information retrieval. </p>
          
       
        </div>

        
        
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">2. The Thought-Retrievers Framework</h2>
        <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/figure2.png" alt="MY ALT TEXT"/>
          </div>
          <b class="subtitle is-size-6">
            Thought-Retriever Framework: (a) Thought retrieval: Upon receiving a user query, Thought-Retriever retrieves top-K data
            chunks from the mixture of external knowledge and thought memory based on embedding similarity; (b) Answer generation: The LLM generates the answer for the user query based on the retrieved data chunks; (c) Thought generation: The LLM further generates thought and its confidence based on the user query and the generated answer; (d) Thought memory update: Meaningless and redundant thoughts are removed and the remaining novel thoughts are used to update the thought memory. 
          </b>
        </div>

        <h2 class="title is-4">a) Thought retrieval</h2>
        <div class="content has-text-justified">
          <p>
            After receiving a user query \(Q_i\), Thought-Retriever \(R\) retrieves relevant information \(T_i\) from
            external knowledge \(K\) and previously generated thought
            memory \(T\) via embedding similarity ranking. This process
            is formulated as \(T_i \leftarrow R(Q_i, K \cup T)\).
          </p>
        </div>

        <h2 class="title is-4">b) Answer Generation</h2>
        <div class="content has-text-justified">
          <p>
            Based on the retrieved information \( \mathcal{T}_{i} \), we design a prompt to combine \( \mathcal{T}_{i} \) and user query \( Q_i \) and feed the prompt to an LLM \( L \) to get the answer \( A_i \). It can be articulated as 
    \( A_i \gets L(Q_i, \mathcal{T}_{i}) \).
          </p>
        </div>
        <h2 class="title is-4">c) Thought Generation</h2>
        <div class="content has-text-justified">
          <p>
            We can generate thoughts via LLM \( L \) using the obtained answer \( A_i \) and its query \( Q_i \). However, redundant or meaningless thoughts during the generation process may harm the LLM performance. To solve this issue, we design a special prompt so that LLM \( L \) can generate thoughts \( T_i \) and thought quality confidence \( c_i \) based on the user's query \( Q_i \) and corresponding answer \( A_i \). This can be described as \( T_i, c_i \gets L(Q_i, A_i) \).
          </p>
        </div>
        <h2 class="title is-4">d) Thought Memory Update</h2>
        <div class="content has-text-justified">
          <p>
            The confidence of thought quality \(c_i\) is a boolean indicator that determines whether the newly generated thought should be updated into the thought memory \(\mathcal{T}\). Here, we design that if the LLM is confident about its answer, where \(c_i\) is True, \(\mathcal{T}\) will be updated.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">3. AcademicEval Benchmark</h2>
        <div class="content">
          <div class="content has-text-justified">
            <p>
              Current benchmarks for assessing agent long-context memory utilization involve tasks such as question-answering,
              long-context summarization, and classification. Despite
              being well-constructed, they are limited in flexibility and
              real-world impact and are costly to acquire. To address these
              issues, we introduce an innovative benchmark, AcademicEval, based on academic papers from arXiv collected on a
              weekly basis. AcademicEval is superior in three aspects:
            </p>
            <ul>
              <li>1) it dynamically collects the most up-to-date data
              </li>
              <li>
                2) it
                acquires high-quality labels at no additional cost
              </li>
              <li>
                3)
                it allows real-world applications with high impacts. AcademicEval comes with two datasets: abstract and related.
    
              </li>
          </ul>

            <p>
              <b>
              For a more detailed introduction to the benchmark and instructions on usage, please refer to this LINK.
              </b>
            </p>

       
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">3. AcademicEval Benchmark</h2>
        <div class="content has-text-justified">
          <p>
            More content here ...
          </p>
        </div> -->
        <!-- <div class="container is-max-desktop" style="margin-bottom: 20px;">  
          <div class="columns">  
              <div class="column is-5">
                  <div class="image">    
                      <img src="static/images/working.png" alt="Image 1"> 
                    </div> 
                  <p style="margin: 10px 0;" class="subtitle is-6 has-text-centered">(a) Configurator</p>  
              </div>
              <div class="column is-1">
            </div>  
              <div class="column is-6">  
                  <div class="image">  
                      <img src="static/images/example.png"  alt="Image 2">  
                  </div>  
                  <p style="margin: 10px 0;" class="subtitle is-6 has-text-centered">(b) A running example</p>  
              </div>  
          </div>  
      </div>   -->
      <!-- Removed the Self-Directed Learning paragraph -->
    <!-- </div>
  </div>
</section> -->

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">4. Experiments</h2>
        <h2 class="title is-4">4.1 Main Results</h2>
        <div class="content has-text-justified">
          <p>
            Experiments on our AcademicEval benchmark and public benchmarks validate that Thought-Retriever remarkably outperforms state-of-the-art baselines.
          </p>
        </div>
        <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/Main_results.png"  alt="MY ALT TEXT"/>
          </div>
  
        </div>
      

        <h2 class="title is-4">4.2 Qualitative Analysis</h2>
        <div class="content has-text-justified">
          <p>
            In addition to the main results, we also identified three interesting and insightful findings that further support the promising nature of our Thought-Retriever framework.
          </p>
        </div>

        <h2 class="title is-5">(a) Self-evolve</h2>
        <div class="content has-text-justified">
          <p>
            <!-- To investigate the relationship between the performance of Thought-Retriever and the number of thoughts, we design an experiment using varying numbers of thoughts on Abstract-multi and Related-multi of AcademicEval. As depicted in Fig (a), there is a distinct trend of increasing F1 scores correlating with the growing number of thoughts, which indicates improved performance. Therefore, more interactions with the users enable Thought-Retriever to assist LLMs in self-evolving and developing deeper understandings, demonstrating a new type of <a href="https://arxiv.org/pdf/2001.08361.pdf"> scailing law </a>.  -->
            The experiment results, shown in Figure (a), demonstrates that more interactions with he users enable Thought-Retriever to assist LLMs in self-evovling and developing deep understanding, demonstrating a new type of <a href="https://arxiv.org/pdf/2001.08361.pdf"> scailing law </a>.
          </p>
        </div>

        <h2 class="title is-5"> (b) Deeper thoughts help abstract queries </h2>
        <div class="content has-text-justified">
          <p>
            <!-- We conduct a case study to explore the relationship between the abstraction levels of queries and the retrieved information. Specifically, we created a set of questions with varying levels of abstraction and ranked them according to their abstraction level using GPT-4. For the retrieved information abstraction level, we first assigned all the raw segments of text from the external knowledge base an abstraction level of 1. The abstraction thought is then calculated as the average abstraction level of all the segments it retrieves, plus one. For example, a thought based solely on the external knowledge base would have an abstraction level of 2. If it also incorporates other thoughts, its abstraction level would be higher. As shown in Fig (b), where the y-axis represents the abstraction level of the question and the x-axis represents the average abstraction level of all information retrieved by our method.  It can be observed that more abstract questions tend to retrieve information with higher abstraction levels. -->

            The experiment illustrated in Figure (b), where the y-axis represents the question's abstraction level and the x-axis denotes the average abstraction level of all information retrieved by our method, demonstrates that Thought-Retriever tends to utilize deeper thoughts when addressing more abstract queries.
          </p>
        </div>

        <h2 class="title is-5"> (c) Better recall and precision balance </h2>
        <div class="content has-text-justified">
          <p>
            <!-- In our motivation example, we showcase where traditional methods fell short in achieving good recall and precision values. Here, we conduct a case study on our Related-multi dataset, where we demonstrate that when compared to other baselines, our Thought-Retriever significantly performs better in balancing recall and precision. 

            In the experiment, the related work section of the case study includes 22 papers, which is regarded as the ground truth of retrieval. We aimed to assess how well different retrievers could cover these 22 papers, given the limitation of retrieving only 8 chunks of information at a time. We plotted the findings in Fig (c), where the x-axis is the recall value and the y-axis represents the precision. It can be observed that all traditional retrieval methods displayed significantly low recall values. This is primarily attributed to the top-K retrieval limit, which, in this scenario, could only encompass at most 8 out of the 22 papers. In comparison, Thought-Retriever demonstrates a notable improvement in recall value. This is because it leverages thoughts which is constructed from multiple papers, thereby allowing Thought-Retriever to achieve a much higher recall. More importantly, Thought-Retriever also exhibits moderately high precision compared to other retrievers. This suggests that, despite a minor trade-off, Thought-Retriever does not significantly compromise its ability to retrieve the most relevant information. -->

            In the motivational example, we highlighted where traditional methods fell short in achieving satisfactory recall and precision values. Here, in the experiment depicted in Figure (c), we demonstrate that, compared with state-of-the-art retrievers, Thought-Retriever, despite a minor trade-off, does not significantly compromise its ability to retrieve the most relevant information (a notable improvement in recall value), thereby achieving a better balance between precision and recall.

          </p>
        </div>

        <div class="content">
          
          
        </div>
        <h2 class="title is-5"></h2>
        <div class="container is-max-desktop" style="margin-bottom: 20px;">  
          <div class="columns">  
              <div class="column is-4">
                  <div class="image">    
                      <img src="static/images/self_evolve.png" alt="Image 1"> 
                    </div> 
                  <p style="margin: 10px 0;" class="subtitle is-6 has-text-centered"><b> (a) Self-evolve</b> </p>  
                  <!-- This figure illustrate the agent's performanc across various datasets as the number of thoughts increases -->
              </div>

             

              <div class="column is-4">
                <div class="image">    
                  <img src="static/images/deepth_abs.png" alt="Image 1"> 
                </div> 
              <p style="margin: 10px 0;" class="subtitle is-6 has-text-centered"> <b> (b) Deeper thoughts help abstract queries </b></p>  
              <!-- This figure il-
                lustrates the correlation between six questions, categorized by their
                level of abstraction as evaluated by GPT-4 (x-axis), and the abstraction level of the corresponding retrieved information (y-axis). -->
              </div>
              
              <!-- <div class="column is-0.2">
              </div> 
           -->
              <div class="column is-4">
                <div class="image">    
                  <img src="static/images/precision_recall.png" alt="Image 1"> 
                </div> 
              <p style="margin: 10px 0;" class="subtitle is-6 has-text-centered"> <b> (b) Better recall and precision balance </b></p>  
              <!-- This figure il-
                lustrates the correlation between six questions, categorized by their
                level of abstraction as evaluated by GPT-4 (x-axis), and the abstraction level of the corresponding retrieved information (y-axis). -->
              </div>

          </div>  

          

  
          
        </div> 
        <div class="content has-text-justified">
          <p>
            More Content here....
          </p>
          
        </div>

        <h2 class="title is-4">4.3 Interaction with other LLMs agent</h2>
        <div class="content has-text-justified">
          <p>
            Forming thoughts can be a lengthy process. When a new agent lacks relevant memory or external knowledge, it is challenging to develop high-quality thoughts and memories from scratch. Consequently, we conducted cases study to show that Thought-Retriever can help the agent quickly learn from other agents who have already formed expert knowledge.
          </p>
        </div>
        <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/MultiAgent.png"  alt="MY ALT TEXT"/>
          </div>
  
        </div>
   

      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        Your image here
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        Your image here
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        Your image here
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
       Your image here
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
       Paper video.
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            Youtube embed code here
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<!-- <section class="section">
  <div class="container is-max-desktop content">
        <h2 class="title">Community</h2>
        <div class="content has-text-justified">
          <p>
            Join our community to connect with other agent enthusiasts, share your tools and demos, and collaborate on exciting initiatives. You can find us on <a href="https://join.slack.com/t/slack-ped8294/shared_invite/zt-2cqebow90-soac9UFKGZ2RcUy8PqjZrA" target="_blank" >Slack</a>.
          </p>
       
      </div>
  </div>
</section> -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        
        <!-- @misc{wu2024oscopilot,
          title={OS-Copilot: Towards Generalist Computer Agents with Self-Improvement}, 
          author={Zhiyong Wu and Chengcheng Han and Zichen Ding and Zhenmin Weng and Zhoumianze Liu and Shunyu Yao and Tao Yu and Lingpeng Kong},
          year={2024},
          eprint={2402.07456},
          archivePrefix={arXiv},
          primaryClass={cs.AI}
        } -->

      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
